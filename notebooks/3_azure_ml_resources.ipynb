{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Azure ML Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "\n",
    "from azure.core.exceptions import ResourceExistsError, ResourceNotFoundError, HttpResponseError\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Workspace\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from azure.ai.ml.entities import AzureBlobDatastore\n",
    "\n",
    "from azure.ai.ml.entities import AmlCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the YAML file\n",
    "with open(\"../config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = config[\"azure\"][\"subscription_id\"]\n",
    "resource_group_name = config[\"azure\"][\"resource_group_name\"]\n",
    "workspace_name = config[\"azure\"][\"workspace_name\"]\n",
    "container_name = config[\"azure\"][\"container_name\"]\n",
    "location = config[\"azure\"][\"location\"]\n",
    "datastore_name = config[\"azure\"][\"datastore_name\"]\n",
    "training_gpu_cluster = config[\"azure\"][\"training_gpu_cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Resource Management client\n",
    "resource_client = ResourceManagementClient(credential, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resource_group(resource_client, resource_group_name, location):\n",
    "    try:\n",
    "        # Intenta obtener el grupo de recursos\n",
    "        resource_group = resource_client.resource_groups.get(resource_group_name)\n",
    "        print(f\"Resource Group '{resource_group_name}' already exists in '{resource_group.location}'.\")\n",
    "    except ResourceNotFoundError:\n",
    "        # Si el grupo de recursos no existe, créalo\n",
    "        resource_group_params = {\"location\": location}\n",
    "        resource_group = resource_client.resource_groups.create_or_update(\n",
    "            resource_group_name,\n",
    "            resource_group_params\n",
    "        )\n",
    "        print(f\"Resource Group '{resource_group_name}' created in '{resource_group.location}'.\")\n",
    "    except Exception as e:\n",
    "        # Maneja otras excepciones\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    return resource_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource Group 'test_group' already exists in 'eastus'.\n"
     ]
    }
   ],
   "source": [
    "# Call the function to create the Resource Group\n",
    "resource_group = create_resource_group(resource_client, resource_group_name, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(credential, subscription_id, resource_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workspace(ml_client, workspace_name, location):\n",
    "    try:\n",
    "        # Try to get the existing Workspace\n",
    "        workspace = ml_client.workspaces.get(workspace_name)\n",
    "        print(f\"Workspace '{workspace_name}' already exists in '{workspace.location}'.\")\n",
    "        return workspace\n",
    "    except ResourceNotFoundError:\n",
    "        # If the Workspace does not exist, create it asynchronously\n",
    "        workspace_poller = ml_client.workspaces.begin_create(\n",
    "            Workspace(\n",
    "                name=workspace_name,\n",
    "                location=location  # Use the 'location' variable\n",
    "            )\n",
    "        )\n",
    "        workspace = workspace_poller.result()  # Wait for the operation to complete\n",
    "        print(f\"Workspace '{workspace_name}' created in '{workspace.location}'.\")\n",
    "        return workspace\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace 'machine_que_tal' already exists in 'eastus'.\n"
     ]
    }
   ],
   "source": [
    "workspace = create_workspace(ml_client, workspace_name, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Woskspace Storage Account Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account_name = workspace.storage_account.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Storage Account Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = StorageManagementClient(credential, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storage_account_keys(storage_client, resource_group_name, account_name):\n",
    "    keys_response = storage_client.storage_accounts.list_keys(resource_group_name, account_name)\n",
    "    keys = {key.key_name: key.value for key in keys_response.keys}\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the storage account keys.\n"
     ]
    }
   ],
   "source": [
    "if storage_account_name:\n",
    "    storage_keys = get_storage_account_keys(storage_client, resource_group_name, storage_account_name)\n",
    "    print(\"Successfully retrieved the storage account keys.\")\n",
    "else:\n",
    "    print(\"Failed to create or retrieve the storage account.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files_to_blob(account_name, account_key, container_name, source_folder):\n",
    "    account_url = f\"https://{account_name}.blob.core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient(account_url=account_url, credential=account_key)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    \n",
    "    try:\n",
    "        container_client.create_container()\n",
    "        print(f\"Container '{container_name}' created.\")\n",
    "    except Exception as e:\n",
    "        if \"ContainerAlreadyExists\" in str(e):\n",
    "            print(f\"Container '{container_name}' already exists.\")\n",
    "        else:\n",
    "            print(f\"Error creating container: {e}\")\n",
    "\n",
    "    files_to_upload = [os.path.join(root, file) for root, dirs, files in os.walk(source_folder) for file in files]\n",
    "    progress_bar = tqdm(files_to_upload)\n",
    "\n",
    "    for file_path in progress_bar:\n",
    "        blob_path = os.path.relpath(file_path, start=source_folder)\n",
    "        blob_client = container_client.get_blob_client(blob_path)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as data:\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "        except Exception as e:\n",
    "            progress_bar.set_description(f\"Failed {os.path.basename(file_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container 'containerdatatrain' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:08<00:00, 10.39it/s]\n"
     ]
    }
   ],
   "source": [
    "account_key = storage_keys['key1']\n",
    "source_folder = '../data/'\n",
    "\n",
    "upload_files_to_blob(storage_account_name, account_key, container_name, source_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Blob Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "ml_client = MLClient(credential, subscription_id, resource_group_name, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datastore(datastore_name, account_name, container_name):\n",
    "    \"\"\"\n",
    "    Ensure a datastore exists in the Azure ML workspace. If it does not exist, create it.\n",
    "\n",
    "    Parameters:\n",
    "    - datastore_name: Name of the datastore to check or create.\n",
    "    - account_name: Azure storage account name associated with the datastore.\n",
    "    - container_name: Azure storage container name associated with the datastore.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the datastore already exists\n",
    "    try:\n",
    "        existing_datastore = ml_client.datastores.get(datastore_name)\n",
    "        print(f\"Datastore '{datastore_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Datastore '{datastore_name}' not found. Creating new datastore.\")\n",
    "        # Create a new datastore if it does not exist\n",
    "        blob_datastore = AzureBlobDatastore(\n",
    "            name=datastore_name,\n",
    "            description=\"Datastore for storing training data and other blobs\",\n",
    "            account_name=account_name,\n",
    "            container_name=container_name,\n",
    "        )\n",
    "\n",
    "        # Register the datastore in the workspace\n",
    "        ml_client.datastores.create_or_update(blob_datastore)\n",
    "        print(f\"Datastore '{datastore_name}' has been created and registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_datastore(datastore_name, storage_account_name, container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Compute Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new gpu compute target...\n",
      "AMLCompute with name training-gpu-cluster is created, the compute size is STANDARD_D2_V3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    gpu_cluster = ml_client.compute.get(training_gpu_cluster)\n",
    "    print(\n",
    "        f\"You already have a cluster named {training_gpu_cluster}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new gpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    gpu_cluster = AmlCompute(\n",
    "        # Name assigned to the compute cluster\n",
    "        name=training_gpu_cluster,\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_D2_V3\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satellite-object-detection-azure-infra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
